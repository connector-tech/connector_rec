{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f5c3db2-f9b9-4032-8464-fda90d9fffe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09294719-fd12-4fca-8ad8-836e044e970b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Yernar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Yernar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Yernar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import json\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def process_and_cluster(json_file, num_clusters=3):\n",
    "    try:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if not isinstance(data, list):\n",
    "            raise ValueError(\"Data is not a list of dictionaries. DataFrame creation not applicable.\")\n",
    "    \n",
    "        df = pd.DataFrame(data)\n",
    "        df.insert(0, 'id', range(1, len(df) + 1))\n",
    "        df.drop(columns=['Religion', 'Activities', 'Favorite sport', 'Dream job', 'Interests'], inplace=True)\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: File not found. {e}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: Invalid JSON data. {e}\")\n",
    "        return None\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "    def preprocess_df(df):\n",
    "        df_text = df.drop(columns=['id'])\n",
    "        df_text = df_text.apply(lambda col: col.astype(str).str.lower())\n",
    "        df_text = df_text.apply(lambda col: col.str.replace(r'[^a-zA-Z\\s]', '', regex=True))\n",
    "        df_text = df_text.apply(lambda col: col.str.replace(r'\\s+', ' ', regex=True))\n",
    "        df_text = df_text.apply(lambda col: col.apply(nltk.word_tokenize))\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        df_text = df_text.apply(lambda col: col.apply(lambda tokens: [token for token in tokens if token not in stop_words]))\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        df_text = df_text.apply(lambda col: col.apply(lambda tokens: [lemmatizer.lemmatize(token, pos=\"v\") for token in tokens]))\n",
    "        df_text = df_text.apply(lambda col: col.apply(' '.join))\n",
    "\n",
    "        preprocessed_df = pd.concat([df[['id']], df_text], axis=1)\n",
    "        return preprocessed_df\n",
    "\n",
    "    preprocessed_df = preprocess_df(df)\n",
    "\n",
    "    def cluster_users_with_cosine_and_kmeans(data_df, parameter_columns, num_clusters=3):\n",
    "        parameter_weights = [1.0] * len(parameter_columns)\n",
    "\n",
    "        def combine_weighted_features(row, weights):\n",
    "            combined_features = []\n",
    "            for i, value in enumerate(row[parameter_columns]):\n",
    "                value = str(value)\n",
    "                weighted_feature = value + ' ' * int(weights[i] * len(value))\n",
    "                combined_features.append(weighted_feature)\n",
    "            return ' '.join(combined_features)\n",
    "\n",
    "        data_df['weighted_features'] = data_df.apply(combine_weighted_features, axis=1, args=[parameter_weights])\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        combined_feature_vectors = vectorizer.fit_transform(data_df['weighted_features'])\n",
    "        \n",
    "        cosine_sim_matrix = cosine_similarity(combined_feature_vectors)\n",
    "        \n",
    "        scaler = MaxAbsScaler()\n",
    "        scaled_cosine_sim_matrix = scaler.fit_transform(cosine_sim_matrix)\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(scaled_cosine_sim_matrix)\n",
    "        \n",
    "        similar_users = []\n",
    "        num_users = len(data_df)\n",
    "        for i in range(num_users):\n",
    "            for j in range(i+1, num_users):\n",
    "                similarity = cosine_sim_matrix[i, j]\n",
    "                user1_id, user2_id = data_df.loc[i, 'id'], data_df.loc[j, 'id']\n",
    "                if (user1_id, user2_id) not in similar_users and (user2_id, user1_id) not in similar_users:\n",
    "                    similar_users.append((user1_id, user2_id, similarity))\n",
    "        \n",
    "        return similar_users, cosine_sim_matrix, cluster_labels\n",
    "\n",
    "    parameter_columns = list(preprocessed_df.columns[1:])\n",
    "    similar_users, cosine_sim_matrix, cluster_labels = cluster_users_with_cosine_and_kmeans(preprocessed_df, parameter_columns, num_clusters)\n",
    "    df_sim = pd.DataFrame(similar_users, columns=['id', 'id2', 'sim'])\n",
    "\n",
    "    # Преобразование DataFrame в JSON\n",
    "    df_sim_json = df_sim.to_json(orient='records')\n",
    "\n",
    "    return df_sim_json\n",
    "\n",
    "\n",
    "# Пример использования функции\n",
    "df_sim_json = process_and_cluster('syntetic_large_en.json', num_clusters=3)\n",
    "#print(df_sim_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76bb06e-6ade-41e1-a325-0c5c5d6fc24e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c0d50aa6-7ed8-4d9e-92cb-ad046377e9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Yernar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Yernar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "[[0.39455116]]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_df(df):\n",
    "    df_text = df.drop(columns=['id'])\n",
    "    df_text = df_text.apply(lambda col: col.astype(str).str.lower())\n",
    "    df_text = df_text.apply(lambda col: col.str.replace(r'[^a-zA-Z\\s]', '', regex=True))\n",
    "    df_text = df_text.apply(lambda col: col.str.replace(r'\\s+', ' ', regex=True))\n",
    "    df_text = df_text.apply(lambda col: col.apply(nltk.word_tokenize))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df_text = df_text.apply(lambda col: col.apply(lambda tokens: [lemmatizer.lemmatize(token, pos=\"v\") for token in tokens]))\n",
    "    df_text = df_text.apply(lambda col: col.apply(' '.join))\n",
    "    preprocessed_df = pd.concat([df[['id']], df_text], axis=1)\n",
    "    return preprocessed_df\n",
    "\n",
    "def cluster_users_with_cosine_and_kmeans(data_df, parameter_columns, num_clusters=2):\n",
    "    parameter_weights = [1.0] * len(parameter_columns)\n",
    "\n",
    "    def combine_weighted_features(row, weights):\n",
    "        combined_features = []\n",
    "        for i, value in enumerate(row[parameter_columns]):\n",
    "            value = str(value)\n",
    "            weighted_feature = value + ' ' * int(weights[i] * len(value))\n",
    "            combined_features.append(weighted_feature)\n",
    "        return ' '.join(combined_features)\n",
    "\n",
    "    data_df['weighted_features'] = data_df.apply(combine_weighted_features, axis=1, args=[parameter_weights])\n",
    "    vectorizer = TfidfVectorizer(min_df=1)\n",
    "    combined_feature_vectors = vectorizer.fit_transform(data_df['weighted_features'])\n",
    "    cosine_sim_matrix = cosine_similarity(combined_feature_vectors)\n",
    "    scaler = MaxAbsScaler()\n",
    "    scaled_cosine_sim_matrix = scaler.fit_transform(cosine_sim_matrix)\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(scaled_cosine_sim_matrix)\n",
    "    similar_users = []\n",
    "    num_users = len(data_df)\n",
    "    for i in range(num_users):\n",
    "        for j in range(i + 1, num_users):\n",
    "            similarity = cosine_sim_matrix[i, j]\n",
    "            user1_id, user2_id = data_df.loc[i, 'id'], data_df.loc[j, 'id']\n",
    "            if (user1_id, user2_id) not in similar_users and (user2_id, user1_id) not in similar_users:\n",
    "                similar_users.append((user1_id, user2_id, similarity))\n",
    "    return similar_users, cosine_sim_matrix, cluster_labels\n",
    "\n",
    "def process_and_train(profile_json, df_act_json, df_sim_json, num_clusters=2):\n",
    "    profile_data = json.loads(profile_json)\n",
    "    \n",
    "    df_list = []\n",
    "    for i, profile in enumerate(profile_data):\n",
    "        profile_df = pd.DataFrame(profile, index=[i])\n",
    "        profile_df.insert(0, 'id', i + 1)\n",
    "        df_list.append(profile_df)\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    act_data = json.loads(df_act_json)\n",
    "    df_act = pd.DataFrame(act_data)\n",
    "    for key in df_act.columns:\n",
    "        df_act[key] = df_act[key].astype(int)\n",
    "\n",
    "    preprocessed_df = preprocess_df(df)\n",
    "    \n",
    "    sim_data = json.loads(df_sim_json)\n",
    "    df_sim = pd.DataFrame(sim_data)\n",
    "    for key in ['id', 'id2']:\n",
    "        df_sim[key] = df_sim[key].astype(int)\n",
    "    df_sim['sim'] = df_sim['sim'].astype(float)\n",
    "\n",
    "    parameter_columns = list(preprocessed_df.columns[1:])\n",
    "    similar_users, cosine_sim_matrix, cluster_labels = cluster_users_with_cosine_and_kmeans(preprocessed_df, parameter_columns, num_clusters)\n",
    "\n",
    "    df_act['Target_ID'] = df_act['Target_ID'].astype(int)\n",
    "    preprocessed_df['id'] = preprocessed_df['id'].astype(int)\n",
    "    df_act['Author_ID'] = df_act['Author_ID'].astype(int)\n",
    "    df_act['Ratio_Duration_Messages'] = 1 / (df_act['Duration_Minutes'] / df_act['Num_Messages'])\n",
    "    merged_df = pd.merge(df_act, df_sim, left_on=['Author_ID', 'Target_ID'], right_on=['id', 'id2'], how='inner')\n",
    "    final_df = pd.merge(preprocessed_df, merged_df, left_on='id', right_on='Author_ID', how='inner')\n",
    "    final_df = pd.merge(final_df, preprocessed_df, left_on='Target_ID', right_on='id', how='inner')\n",
    "    author_ids = final_df['Author_ID']\n",
    "    target_ids = final_df['Target_ID']\n",
    "    final_df.drop(columns=['Author_ID', 'Target_ID', 'id_y', 'id2'], inplace=True)\n",
    "    final_df.rename(columns={'id_x': 'Author_ID'}, inplace=True)\n",
    "    final_df.drop(columns=['Author_ID', 'id'], inplace=True)\n",
    "\n",
    "    vectorizers = {}\n",
    "    X_text_features = []\n",
    "    text_columns = ['Goals_x', 'Personality traits_x', 'Dreams and goals_x',\n",
    "                    'Thoughts on life_x', 'Expectations_x', 'weighted_features_x',\n",
    "                    'Goals_y', 'Personality traits_y', 'Dreams and goals_y',\n",
    "                    'Thoughts on life_y', 'Expectations_y', 'weighted_features_y']\n",
    "\n",
    "    for col in text_columns:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X_text_feature = vectorizer.fit_transform(final_df[col])\n",
    "        vectorizers[col] = vectorizer\n",
    "        X_text_features.append(X_text_feature.toarray())\n",
    "\n",
    "    X_text_features = np.concatenate(X_text_features, axis=1)\n",
    "    X_numerical = final_df.drop(columns=text_columns).values\n",
    "    X = np.concatenate((X_text_features, X_numerical), axis=1)\n",
    "    y = final_df['Ratio_Duration_Messages'].values\n",
    "    scaler1 = MinMaxScaler()\n",
    "    y = scaler1.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "    X_train, X_test, y_train, y_test, author_ids_train, author_ids_test, target_ids_train, target_ids_test = train_test_split(\n",
    "        X, y, author_ids, target_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "   \n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    print(predictions)\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'Author_ID': author_ids_test,\n",
    "        'Target_ID': target_ids_test,\n",
    "        'Prediction': predictions.flatten()\n",
    "    })\n",
    "\n",
    "    results_json = results_df.to_json(orient='records')\n",
    "    return results_json\n",
    "\n",
    "\n",
    "'''\n",
    "profile_json = json.dumps([\n",
    "    {\n",
    "        \"Goals\": \"Christian gay rasizms you bro somersimte saddnes sun time clock reading sunset\",\n",
    "        \"Personality traits\": \"Christian gay rasizms you bro somersimte saddnes sun time clock reading sunset\",\n",
    "        \"Dreams and goals\": \"Christian gay rasizms you bro somersimte saddnes sun time clock reading sunset\",\n",
    "        \"Thoughts on life\": \"Christian gay rasizms you bro somersimte saddnes sun time clock reading sunset\",\n",
    "        \"Expectations\": \"Christian gay rasizms you bro somersimte saddnes sun time clock reading sunset\"\n",
    "    },\n",
    "    {\n",
    "        \"Goals\": \"Another line of goals\",\n",
    "        \"Personality traits\": \"Another line of personality traits\",\n",
    "        \"Dreams and goals\": \"Another line of dreams and goals\",\n",
    "        \"Thoughts on life\": \"Another line of thoughts on life\",\n",
    "        \"Expectations\": \"Another line of expectations\"\n",
    "    },\n",
    "    {\n",
    "        \"Goals\": \"More goals for testing\",\n",
    "        \"Personality traits\": \"More personality traits for testing\",\n",
    "        \"Dreams and goals\": \"More dreams and goals for testing\",\n",
    "        \"Thoughts on life\": \"More thoughts on life for testing\",\n",
    "        \"Expectations\": \"More expectations for testing\"\n",
    "    }\n",
    "])\n",
    "\n",
    "df_act = json.dumps({\n",
    "    \"Author_ID\": [\"1\", \"2\", \"3\"],\n",
    "    \"Target_ID\": [\"2\", \"3\", \"1\"],\n",
    "    \"Num_Messages\": [\"10\", \"15\", \"8\"],\n",
    "    \"Duration_Minutes\": [\"30\", \"45\", \"25\"]\n",
    "})\n",
    "\n",
    "df_sim = json.dumps({\n",
    "    \"id\": [\"1\", \"2\", \"3\"],\n",
    "    \"id2\": [\"2\", \"3\", \"1\"],\n",
    "    \"sim\": [\"0.8\", \"0.85\", \"0.75\"]\n",
    "})\n",
    "'''\n",
    "results_json = process_and_train(profile_json, df_act, df_sim, num_clusters=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b5d7a7-07b6-4486-8a2c-e1f5d50d1219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_df(df):\n",
    "    df_text = df.drop(columns=['id'])\n",
    "    df_text = df_text.apply(lambda col: col.astype(str).str.lower())\n",
    "    df_text = df_text.apply(lambda col: col.str.replace(r'[^a-zA-Z\\s]', '', regex=True))\n",
    "    df_text = df_text.apply(lambda col: col.str.replace(r'\\s+', ' ', regex=True))\n",
    "    df_text = df_text.apply(lambda col: col.apply(nltk.word_tokenize))\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df_text = df_text.apply(lambda col: col.apply(lambda tokens: [token for token in tokens if token not in stop_words]))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df_text = df_text.apply(lambda col: col.apply(lambda tokens: [lemmatizer.lemmatize(token, pos=\"v\") for token in tokens]))\n",
    "    df_text = df_text.apply(lambda col: col.apply(' '.join))\n",
    "    preprocessed_df = pd.concat([df[['id']], df_text], axis=1)\n",
    "    return preprocessed_df\n",
    "\n",
    "def cluster_users_with_cosine_and_kmeans(data_df, parameter_columns, num_clusters=2):\n",
    "    parameter_weights = [1.0] * len(parameter_columns)\n",
    "\n",
    "    def combine_weighted_features(row, weights):\n",
    "        combined_features = []\n",
    "        for i, value in enumerate(row[parameter_columns]):\n",
    "            value = str(value)\n",
    "            weighted_feature = value + ' ' * int(weights[i] * len(value))\n",
    "            combined_features.append(weighted_feature)\n",
    "        return ' '.join(combined_features)\n",
    "\n",
    "    data_df['weighted_features'] = data_df.apply(combine_weighted_features, axis=1, args=[parameter_weights])\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    combined_feature_vectors = vectorizer.fit_transform(data_df['weighted_features'])\n",
    "    cosine_sim_matrix = cosine_similarity(combined_feature_vectors)\n",
    "    scaler = MaxAbsScaler()\n",
    "    scaled_cosine_sim_matrix = scaler.fit_transform(cosine_sim_matrix)\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(scaled_cosine_sim_matrix)\n",
    "    similar_users = []\n",
    "    num_users = len(data_df)\n",
    "    for i in range(num_users):\n",
    "        for j in range(i+1, num_users):\n",
    "            similarity = cosine_sim_matrix[i, j]\n",
    "            user1_id, user2_id = data_df.loc[i, 'id'], data_df.loc[j, 'id']\n",
    "            if (user1_id, user2_id) not in similar_users and (user2_id, user1_id) not in similar_users:\n",
    "                similar_users.append((user1_id, user2_id, similarity))\n",
    "    return similar_users, cosine_sim_matrix, cluster_labels\n",
    "\n",
    "def main():\n",
    "    # Generate synthetic activity data\n",
    "    activity_data = generate_data(150)\n",
    "    df_act = pd.DataFrame(activity_data, columns=['Author_ID', 'Target_ID', 'Num_Messages', 'Duration_Minutes'])\n",
    "\n",
    "    # Load and preprocess synthetic profile data\n",
    "    with open('syntetic_large_en.json', 'r', encoding='utf-8') as f:\n",
    "        profile_data = json.load(f)\n",
    "    df = pd.DataFrame(profile_data)\n",
    "    df.insert(0, 'id', range(1, len(df) + 1))\n",
    "    df.drop(columns=['Religion', 'Activities', 'Favorite sport', 'Dream job', 'Interests'], inplace=True)\n",
    "    preprocessed_df = preprocess_df(df)\n",
    "\n",
    "    # Cluster users and calculate similarities\n",
    "    parameter_columns = list(preprocessed_df.columns[1:])\n",
    "    similar_users, cosine_sim_matrix, cluster_labels = cluster_users_with_cosine_and_kmeans(preprocessed_df, parameter_columns, num_clusters=3)\n",
    "    df_sim = pd.DataFrame(similar_users, columns=['id', 'id2', 'sim'])\n",
    "\n",
    "    # Merge and prepare data for training\n",
    "    df_act['Target_ID'] = df_act['Target_ID'].astype(int)\n",
    "    preprocessed_df['id'] = preprocessed_df['id'].astype(int)\n",
    "    df_act['Author_ID'] = df_act['Author_ID'].astype(int)\n",
    "    df_sim['id'] = df_sim['id'].astype(int)\n",
    "    df_sim['id2'] = df_sim['id2'].astype(int)\n",
    "    df_act['Ratio_Duration_Messages'] = 1 / (df_act['Duration_Minutes'] / df_act['Num_Messages'])\n",
    "    merged_df = pd.merge(df_act, df_sim, left_on=['Author_ID', 'Target_ID'], right_on=['id', 'id2'], how='inner')\n",
    "    final_df = pd.merge(preprocessed_df, merged_df, left_on='id', right_on='Author_ID', how='inner')\n",
    "    final_df = pd.merge(final_df, preprocessed_df, left_on='Target_ID', right_on='id', how='inner')\n",
    "    author_ids = final_df['Author_ID']\n",
    "    target_ids = final_df['Target_ID']\n",
    "    final_df.drop(columns=['Author_ID', 'Target_ID', 'id_y', 'id2'], inplace=True)\n",
    "    final_df.rename(columns={'id_x': 'Author_ID'}, inplace=True)\n",
    "    final_df.drop(columns=['Author_ID', 'id'], inplace=True)\n",
    "\n",
    "    vectorizers = {}\n",
    "    X_text_features = []\n",
    "    text_columns = ['Goals_x', 'Personality traits_x', 'Dreams and goals_x',\n",
    "                    'Thoughts on life_x', 'Expectations_x', 'weighted_features_x',\n",
    "                    'Goals_y', 'Personality traits_y', 'Dreams and goals_y',\n",
    "                    'Thoughts on life_y', 'Expectations_y', 'weighted_features_y']\n",
    "\n",
    "    for col in text_columns:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X_text_feature = vectorizer.fit_transform(final_df[col])\n",
    "        vectorizers[col] = vectorizer\n",
    "        X_text_features.append(X_text_feature.toarray())\n",
    "\n",
    "    X_text_features = np.concatenate(X_text_features, axis=1)\n",
    "    X_numerical = final_df.drop(columns=text_columns).values\n",
    "    X = np.concatenate((X_text_features, X_numerical), axis=1)\n",
    "    y = final_df['Ratio_Duration_Messages'].values\n",
    "    scaler1 = MinMaxScaler()\n",
    "    y = scaler1.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "    X_train, X_test, y_train, y_test, author_ids_train, author_ids_test, target_ids_train, target_ids_test = train_test_split(\n",
    "        X, y, author_ids, target_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create and train the neural network model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'Author_ID': author_ids_test,\n",
    "        'Target_ID': target_ids_test,\n",
    "        'Prediction': predictions.flatten()\n",
    "    })\n",
    "\n",
    "    results_json = results_df.to_json(orient='records')\n",
    "    return results_json\n",
    "\n",
    "# Пример использования функции\n",
    "def generate_data(num_rows):\n",
    "    data = []\n",
    "    author_ids = list(range(1, 101))  # 100 различных числовых идентификаторов для авторов\n",
    "    target_ids = list(range(1, 101))  # 100 различных числовых идентификаторов для целей\n",
    "    \n",
    "    for _ in range(num_rows):\n",
    "        author = random.choice(author_ids)\n",
    "        target = random.choice(target_ids)\n",
    "        num_messages = random.randint(1, 20)  # Случайное количество сообщений\n",
    "        first_message_time = random_date(datetime(2024, 1, 1), datetime(2024, 4, 1))  # Случайное время первого сообщения\n",
    "        last_message_time = first_message_time + timedelta(minutes=random.randint(1, 60*num_messages))  # Время последнего сообщения\n",
    "        duration_minutes = int((last_message_time - first_message_time).total_seconds() / 60)  # Продолжительность общения в минутах\n",
    "        \n",
    "        data.append([author, target, num_messages, duration_minutes])\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Создание DataFrame\n",
    "data = generate_data(150)\n",
    "df_act = pd.DataFrame(data, columns=['Author_ID', 'Target_ID', 'Num_Messages', 'Duration_Minutes'])\n",
    "results_json = process_and_train('syntetic_large_en.json', df_act, num_clusters=3)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
